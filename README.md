# üõí DealHunter Pro

Sistema inteligente de web scraping e enriquecimento ass√≠ncrono de ofertas do Mercado Livre com alta performance e escalabilidade.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Funcionalidades](#funcionalidades)
- [Requisitos](#requisitos)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Uso](#uso)
- [Performance e Otimiza√ß√µes](#performance-e-otimiza√ß√µes)
- [Monitoramento](#monitoramento)
- [Testes](#testes)
- [Deploy](#deploy)
- [Troubleshooting](#troubleshooting)

## üéØ Vis√£o Geral

DealHunter Pro √© uma plataforma de scraping otimizada que:

- üöÄ **Coleta ofertas** do Hub de Afiliados do Mercado Livre
- üéØ **Filtra** produtos por desconto m√≠nimo configur√°vel
- ‚ö° **Enriquece** ofertas assincronamente com dados completos
- üíæ **Armazena** tudo em banco Supabase para an√°lise
- üìä **Monitora** performance com Prometheus e Grafana
- üîß **Escala** horizontalmente com m√∫ltiplos workers

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Scraper   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Filter     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Database    ‚îÇ
‚îÇ (Playwright)‚îÇ     ‚îÇ (Desconto)   ‚îÇ     ‚îÇ  (Supabase)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                         ‚îÇ  Redis Queue   ‚îÇ
                                         ‚îÇ   (RQ Jobs)    ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚ñº                         ‚ñº                         ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Worker 1 ‚îÇ             ‚îÇ Worker 2 ‚îÇ      ...    ‚îÇ Worker N ‚îÇ
                  ‚îÇ(Browser  ‚îÇ             ‚îÇ(Browser  ‚îÇ             ‚îÇ(Browser  ‚îÇ
                  ‚îÇ  Pool)   ‚îÇ             ‚îÇ  Pool)   ‚îÇ             ‚îÇ  Pool)   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ                         ‚îÇ                         ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                         ‚îÇ  Update DB     ‚îÇ
                                         ‚îÇ (Enrichment)   ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Principais

1. **Scraper**: Coleta inicial de ofertas usando Playwright
2. **Filter**: Aplica regras de neg√≥cio (desconto m√≠nimo)
3. **Database**: Persist√™ncia em Supabase com hist√≥rico
4. **Queue**: Enfileiramento em lote com Redis
5. **Workers**: Processamento paralelo com browser pool
6. **Monitoring**: M√©tricas Prometheus e dashboards Grafana

## ‚ú® Funcionalidades

### Core Features

- ‚úÖ Scraping com scroll infinito inteligente
- ‚úÖ Extra√ß√£o robusta de pre√ßos e descontos
- ‚úÖ Browser pool para performance (5-10x mais r√°pido)
- ‚úÖ Enfileiramento em lote com Redis pipeline
- ‚úÖ Retry autom√°tico com backoff exponencial
- ‚úÖ Rate limiting para evitar bloqueios
- ‚úÖ Circuit breaker para prote√ß√£o contra falhas
- ‚úÖ Health checks e graceful shutdown
- ‚úÖ Conex√£o thread-safe ao banco de dados

### Otimiza√ß√µes de Performance

| Otimiza√ß√£o      | Ganho Estimado | Descri√ß√£o                            |
| --------------- | -------------- | ------------------------------------ |
| Browser Pool    | **10x**        | Reutiliza√ß√£o de contextos Playwright |
| Batch Enqueuing | **20x**        | Redis pipeline para enfileiramento   |
| √çndices DB      | **5x**         | Queries otimizadas com √≠ndices       |
| Async/Sync Fix  | **30%**        | Event loop √∫nico e eficiente         |
| Thread-safe DB  | N/A            | Evita race conditions                |

## üì¶ Requisitos

- Python 3.13+
- Redis 7+
- Supabase (PostgreSQL)
- Docker & Docker Compose (opcional)

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Docker (Recomendado)

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/dealhunter-pro.git
cd dealhunter-pro

# Configure o .env
cp .env.development .env
# Edite .env com suas credenciais

# Inicie todos os servi√ßos
docker-compose up -d

# Verifique os logs
docker-compose logs -f
```

### Op√ß√£o 2: Instala√ß√£o Local

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/dealhunter-pro.git
cd dealhunter-pro

# Crie ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale depend√™ncias de produ√ß√£o
pip install -r requirements.txt

# Ou instale depend√™ncias de desenvolvimento (inclui testes)
pip install -r requirements-dev.txt

# Instale Playwright browsers
playwright install chromium

# Configure o .env
cp .env.development .env
# Edite .env com suas credenciais
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

Copie `.env.development` para `.env` e configure:

```bash
# Ambiente
ENVIRONMENT=development  # development, staging, production

# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-key-here

# Redis
REDIS_URL=redis://localhost:6379/0

# Scraping
ML_MAX_SCROLLS=4
ML_SCROLL_DELAY_S=0.45
MIN_DISCOUNT_PCT=50

# Enrichment
ENRICHMENT_WORKER_CONCURRENCY=3
ENRICHMENT_REQUEST_DELAY_S=0.5
```

### Configura√ß√£o por Ambiente

O sistema suporta configura√ß√µes espec√≠ficas por ambiente:

- **Development**: Debug ativo, poucos workers, m√©tricas desabilitadas
- **Staging**: Configura√ß√£o intermedi√°ria para testes
- **Production**: Otimizado para m√°xima performance

Veja [src/shared/config/environments.py](src/shared/config/environments.py) para detalhes.

### Migrations de Banco de Dados

Execute as migrations para criar √≠ndices de performance:

```bash
# Via SQL Editor do Supabase
# 1. Acesse: https://supabase.com/dashboard/project/SEU_PROJETO
# 2. SQL Editor ‚Üí New Query
# 3. Copie o conte√∫do de migrations/005_add_performance_indexes.sql
# 4. Cole no editor e execute (Run)
```

## üéÆ Uso

### Modo Local

```bash
# Inicie Redis
redis-server

# Terminal 1: Execute scraping
# A partir da raiz do projeto
python src/main.py

# Terminal 2: Inicie workers
# Op√ß√£o 1: Com PYTHONPATH (recomendado)
PYTHONPATH=src python -m adapters.workers.enrichment_worker

# Op√ß√£o 2: Executar diretamente
python src/adapters/workers/enrichment_worker.py

# Op√ß√£o 3: Exportar PYTHONPATH permanentemente (opcional)
export PYTHONPATH=src
python -m adapters.workers.enrichment_worker 

# Terminal 3: Inicie RQ Dashboard (opcional)
# Op√ß√£o 1: Com PYTHONPATH (recomendado)
PYTHONPATH=src python -m adapters.workers.start_dashboard

# Op√ß√£o 2: Executar diretamente
python src/adapters/workers/start_dashboard.py

# Op√ß√£o 3: Exportar PYTHONPATH permanentemente (opcional)
export PYTHONPATH=src
python -m adapters.workers.start_dashboard

# Acesse: http://localhost:9181
```

### Modo Docker

```bash
# Inicie tudo
docker-compose up -d

# Execute scraping
docker-compose run --rm app python src/main.py

# Escalone workers
docker-compose up -d --scale worker=5

# Veja dashboard
open http://localhost:9181
```

### Com Monitoramento

```bash
# Inicie com Prometheus e Grafana
docker-compose --profile monitoring up -d

# Acesse:
# - RQ Dashboard: http://localhost:9181
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3000 (admin/admin)
```

## üèéÔ∏è Performance e Otimiza√ß√µes

### Browser Pool

Configur√°vel via `BROWSER_POOL_SIZE` (padr√£o: 3)

```python
# Uso autom√°tico no enrichment_service
from core.use_cases.enrichment_service import enrich_offer

result = await enrich_offer(
    url=url,
    use_browser_pool=True  # 10x mais r√°pido
)
```

### Batch Operations

Enfileiramento em lote automaticamente ativo:

```python
# Antes: 100 ofertas = 200 queries + 100 Redis commands
# Depois: 100 ofertas = 2 queries + 1 Redis pipeline
```

### Rate Limiting

Prote√ß√£o autom√°tica contra bloqueios:

```python
# Configura√ß√£o padr√£o: 10 req/min para ML
# Ajuste em src/shared/utils/rate_limiter.py
```

### Circuit Breaker

Prote√ß√£o contra falhas em cascata:

- Abre ap√≥s 5 falhas consecutivas
- Fecha ap√≥s 2 sucessos
- Timeout de 60s

## üìä Monitoramento

### M√©tricas Dispon√≠veis

```
# Scraping
dealhunter_scrape_duration_seconds
dealhunter_offers_collected_total
dealhunter_scraping_errors_total

# Enrichment
dealhunter_enrichment_duration_seconds
dealhunter_enrichment_success_total
dealhunter_jobs_enqueued_total

# Workers
dealhunter_active_workers
dealhunter_queue_size

# Database
dealhunter_database_queries_total
dealhunter_database_query_duration_seconds

# Rate Limiting
dealhunter_rate_limit_hit_total
dealhunter_circuit_breaker_state
```

### Dashboards

Acesse Grafana em `http://localhost:3000` para visualizar:

- Performance de scraping
- Throughput de workers
- Lat√™ncia de enriquecimento
- Estado de circuit breakers
- Tamanho de filas

## üß™ Testes

```bash
# Execute todos os testes
pytest

# Com cobertura (estrutura nova)
pytest --cov=src --cov-report=html

# Com cobertura (estrutura antiga - compatibilidade)
pytest --cov=app --cov-report=html

# Apenas testes r√°pidos
pytest -m "not slow"

# Testes espec√≠ficos
pytest tests/test_utils.py
```

## üö¢ Deploy

### Produ√ß√£o

```bash
# 1. Configure vari√°veis de produ√ß√£o
cp .env.production .env
# Edite com credenciais reais

# 2. Build da imagem
docker build -t dealhunter-pro:latest .

# 3. Push para registry
docker tag dealhunter-pro:latest registry.com/dealhunter-pro:latest
docker push registry.com/dealhunter-pro:latest

# 4. Deploy com docker-compose
ENVIRONMENT=production docker-compose up -d --scale worker=10
```

### Escalabilidade

**Workers**: Escale horizontalmente conforme necess√°rio

```bash
# 10 workers paralelos
docker-compose up -d --scale worker=10

# Ou via Kubernetes
kubectl scale deployment dealhunter-worker --replicas=20
```

**Redis**: Use Redis Cluster para alta disponibilidade

**Database**: Supabase escala automaticamente

## üîß Troubleshooting

### Redis n√£o conecta

```bash
# Verifique se Redis est√° rodando
redis-cli ping
# PONG

# Teste conex√£o
redis-cli -u redis://localhost:6379/0
```

### Workers n√£o processam jobs

```bash
# Verifique logs
docker-compose logs worker

# Verifique fila
redis-cli
> LLEN "rq:queue:enrichment"

# Limpe fila
> DEL "rq:queue:enrichment"
```

### Scraping muito lento

1. Aumente `BROWSER_POOL_SIZE`
2. Reduza `ENRICHMENT_REQUEST_DELAY_S`
3. Escale workers: `docker-compose up -d --scale worker=5`

### Rate limit do ML

1. Aumente `ML_SCROLL_DELAY_S`
2. Reduza `ENRICHMENT_WORKER_CONCURRENCY`
3. Verifique m√©tricas de rate limiting

## üìö Documenta√ß√£o Adicional

- [ENRICHMENT_README.md](ENRICHMENT_README.md) - Detalhes do sistema de enriquecimento
- [FASE6_RELATORIO_FINAL.md](FASE6_RELATORIO_FINAL.md) - Relat√≥rio completo da refatora√ß√£o
- [migrations/](migrations/) - Scripts de banco de dados

### Estrutura do C√≥digo

O projeto foi refatorado seguindo **Clean Architecture**:

- **`src/core/`**: Regras de neg√≥cio puras (domain + use_cases)
- **`src/adapters/`**: Integra√ß√µes externas (database, external APIs, queues, workers)
- **`src/shared/`**: Recursos compartilhados (config, constants, utils)

### Exemplos de Imports

**Antes (estrutura antiga):**
```python
from config import get_config
from models import ScrapedOffer
from services import ScrapeService
```

**Depois (estrutura nova):**
```python
from shared.config.settings import get_config
from core.domain import ScrapedOffer
from core.use_cases.scrape_service import ScrapeService
```

Veja [FASE6_RELATORIO_FINAL.md](FASE6_RELATORIO_FINAL.md) para detalhes completos da refatora√ß√£o.

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-feature`)
3. Commit suas mudan√ßas (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## üìû Suporte

- Issues: https://github.com/seu-usuario/dealhunter-pro/issues
- Email: seu-email@exemplo.com

---

**Desenvolvido com ‚ù§Ô∏è e otimizado para performance m√°xima**
